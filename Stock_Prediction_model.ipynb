{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YulM3uAC01ky",
        "outputId": "45e657c1-c7d1-433b-b271-bdc869bebef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.8.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install praw\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install nltk\n",
        "!pip install textblob\n",
        "!pip install matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "# Reddit API credentials\n",
        "reddit = praw.Reddit(\n",
        "    client_id='Bg1PYjd4bCnygBrVzK-LsQ',\n",
        "    client_secret='u3PPcsF7XhhsepKP5Uh_m6LpwneF6Q',\n",
        "    user_agent=\"Prem by /u/Over-Fact1365\"\n",
        "\n",
        ")\n",
        "\n",
        "# Define subreddit and fetch posts\n",
        "subreddit = reddit.subreddit('stocks')  # Replace 'stocks' with your desired subreddit\n",
        "posts = subreddit.hot(limit=100)  # Fetch the top 100 \"hot\" posts\n",
        "\n",
        "# Process the posts\n",
        "data = []\n",
        "for post in posts:\n",
        "    data.append({\n",
        "        'Title': post.title,\n",
        "        'Score': post.score,\n",
        "        'Comments': post.num_comments,\n",
        "        'URL': post.url\n",
        "    })\n",
        "\n",
        "# Save the data to a CSV\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('reddit_data.csv', index=False)\n",
        "print(\"Data saved to reddit_data.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BolVK0_1mTT",
        "outputId": "10739f63-c65c-47df-b613-38b1738285e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to reddit_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('reddit_data.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "bkbOii4Z5UPb",
        "outputId": "64757197-d339-479d-b115-588044024d35"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4f39b0df-e1f5-4e9e-8bbb-1dab230dfb14\", \"reddit_data.csv\", 15681)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('reddit_data.csv')\n",
        "\n",
        "print(df.head())\n",
        "print(df.isnull().sum())\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgpPWb9y5rJu",
        "outputId": "fb67005f-e480-4c34-b299-fceec1b1de22"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Title  Score  Comments  \\\n",
            "0  r/Stocks Daily Discussion & Fundamentals Frida...     21       424   \n",
            "1  r/Stocks Daily Discussion Wednesday - Nov 20, ...      7       273   \n",
            "2  Nvidia Earnings: Beat Across the Board, Strong...    363       193   \n",
            "3  Target shares plunge 20% after discounter cuts...    536       291   \n",
            "4                              Snowflake Q3 Earnings     43        11   \n",
            "\n",
            "                                                 URL  \n",
            "0  https://www.reddit.com/r/stocks/comments/1grtf...  \n",
            "1  https://www.reddit.com/r/stocks/comments/1gvmi...  \n",
            "2  https://www.reddit.com/r/stocks/comments/1gvzy...  \n",
            "3  https://www.reddit.com/r/stocks/comments/1gvrc...  \n",
            "4  https://www.reddit.com/r/stocks/comments/1gvzu...  \n",
            "Title       0\n",
            "Score       0\n",
            "Comments    0\n",
            "URL         0\n",
            "dtype: int64\n",
            "            Score    Comments\n",
            "count  100.000000  100.000000\n",
            "mean   109.130000   74.950000\n",
            "std    157.070187   95.321505\n",
            "min      0.000000    0.000000\n",
            "25%      9.000000   12.000000\n",
            "50%     33.000000   41.000000\n",
            "75%    147.000000   85.750000\n",
            "max    769.000000  424.000000\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import download\n",
        "import pandas as pd\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "download('stopwords')\n",
        "download('punkt')\n",
        "download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)  # Remove special characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    tokens = word_tokenize(text)  # Tokenize the text\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words(\"english\")]  # Remove stopwords and lemmatize\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Assuming 'df' is your DataFrame loaded from 'reddit_data.csv'\n",
        "df = pd.read_csv('reddit_data.csv') # Ensure df is loaded or created\n",
        "\n",
        "# Apply preprocessing to Title and Content or selftext if available\n",
        "df['cleaned_title'] = df['Title'].apply(preprocess_text)\n",
        "# Choose one of the options below based on your data:\n",
        "# Option 1: If you want to clean and use the 'selftext' column (if it exists)\n",
        "if 'selftext' in df.columns:\n",
        "    df['cleaned_content'] = df['selftext'].apply(preprocess_text)\n",
        "# Option 2: If you want to reuse the cleaned titles for 'cleaned_content'\n",
        "else:\n",
        "    df['cleaned_content'] = df['Title'].apply(preprocess_text)\n",
        "\n",
        "# Check the cleaned data\n",
        "print(df[['cleaned_title', 'cleaned_content']].head())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhLyC6qQ9XaV",
        "outputId": "8bde7f57-7c33-4e81-bf92-6e952c60c314"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       cleaned_title  \\\n",
            "0    rstocks daily discussion fundamental friday nov   \n",
            "1             rstocks daily discussion wednesday nov   \n",
            "2  nvidia earnings beat across board strong guidance   \n",
            "3  target share plunge discounter cut forecast po...   \n",
            "4                               snowflake q earnings   \n",
            "\n",
            "                                     cleaned_content  \n",
            "0    rstocks daily discussion fundamental friday nov  \n",
            "1             rstocks daily discussion wednesday nov  \n",
            "2  nvidia earnings beat across board strong guidance  \n",
            "3  target share plunge discounter cut forecast po...  \n",
            "4                               snowflake q earnings  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Sentiment analysis function\n",
        "def get_sentiment(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 0\n",
        "    analysis = TextBlob(text)\n",
        "    return analysis.sentiment.polarity  # Polarity ranges from -1 (negative) to 1 (positive)\n",
        "\n",
        "# Calculate sentiment scores\n",
        "df['title_sentiment'] = df['cleaned_title'].apply(get_sentiment)\n",
        "df['content_sentiment'] = df['cleaned_content'].apply(get_sentiment)\n",
        "\n",
        "# Inspect sentiment scores\n",
        "print(df[['cleaned_title', 'title_sentiment', 'cleaned_content', 'content_sentiment']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvhbwlBq7jT3",
        "outputId": "2ab19bb4-542d-4102-98ab-9867c6b6ea47"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       cleaned_title  title_sentiment  \\\n",
            "0    rstocks daily discussion fundamental friday nov         0.000000   \n",
            "1             rstocks daily discussion wednesday nov         0.000000   \n",
            "2  nvidia earnings beat across board strong guidance         0.433333   \n",
            "3  target share plunge discounter cut forecast po...         0.000000   \n",
            "4                               snowflake q earnings         0.000000   \n",
            "\n",
            "                                     cleaned_content  content_sentiment  \n",
            "0    rstocks daily discussion fundamental friday nov           0.000000  \n",
            "1             rstocks daily discussion wednesday nov           0.000000  \n",
            "2  nvidia earnings beat across board strong guidance           0.433333  \n",
            "3  target share plunge discounter cut forecast po...           0.000000  \n",
            "4                               snowflake q earnings           0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine title and content sentiment\n",
        "df['average_sentiment'] = (df['title_sentiment'] + df['content_sentiment']) / 2\n",
        "\n",
        "# Create binary sentiment label\n",
        "df['sentiment_label'] = df['average_sentiment'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Check the final features\n",
        "print(df[['Score', 'Comments', 'average_sentiment', 'sentiment_label']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO1XnTY7-BRP",
        "outputId": "fdf21eab-6900-44ee-8d15-996b7309e14e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Score  Comments  average_sentiment  sentiment_label\n",
            "0     21       424           0.000000                0\n",
            "1      7       273           0.000000                0\n",
            "2    363       193           0.433333                1\n",
            "3    536       291           0.000000                0\n",
            "4     43        11           0.000000                0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance\n",
        "import yfinance as yf\n",
        "\n",
        "# Fetch historical stock data (example: Tesla)\n",
        "stock_data = yf.download(\"TSLA\", start=\"2023-01-01\", end=\"2023-12-31\")\n",
        "print(stock_data.head())\n",
        "\n",
        "# Combine Reddit data and stock data based on timestamps\n",
        "# Ensure that your Reddit data has timestamps and align them properly.\n",
        "# You may need to preprocess the 'created_utc' column from Reddit scraping (not included in earlier code).\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueSQYw3a-j99",
        "outputId": "e1658058-8404-449a-e132-71ee362fb741"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.49)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (5.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2024.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.8.30)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Price                       Adj Close       Close        High         Low  \\\n",
            "Ticker                           TSLA        TSLA        TSLA        TSLA   \n",
            "Date                                                                        \n",
            "2023-01-03 00:00:00+00:00  108.099998  108.099998  118.800003  104.639999   \n",
            "2023-01-04 00:00:00+00:00  113.639999  113.639999  114.589996  107.519997   \n",
            "2023-01-05 00:00:00+00:00  110.339996  110.339996  111.750000  107.160004   \n",
            "2023-01-06 00:00:00+00:00  113.059998  113.059998  114.389999  101.809998   \n",
            "2023-01-09 00:00:00+00:00  119.769997  119.769997  123.519997  117.110001   \n",
            "\n",
            "Price                            Open     Volume  \n",
            "Ticker                           TSLA       TSLA  \n",
            "Date                                              \n",
            "2023-01-03 00:00:00+00:00  118.470001  231402800  \n",
            "2023-01-04 00:00:00+00:00  109.110001  180389000  \n",
            "2023-01-05 00:00:00+00:00  110.510002  157986300  \n",
            "2023-01-06 00:00:00+00:00  103.000000  220911100  \n",
            "2023-01-09 00:00:00+00:00  118.959999  190284000  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Features and target\n",
        "features = df[['Score', 'Comments', 'average_sentiment']]\n",
        "target = df['sentiment_label']  # Replace this with actual stock movement labels if available\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYjcUrZ5-nB-",
        "outputId": "8843f97b-9caa-44f0-8f3e-8d50f3b7a312"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        18\n",
            "           1       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(model, 'stock_prediction_model.pkl')\n",
        "print(\"Model saved as stock_prediction_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCR6SZF4-uN8",
        "outputId": "dbd7a136-b063-4b73-dca8-72cbe411c1e8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as stock_prediction_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ldNkpXmi-y0v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}